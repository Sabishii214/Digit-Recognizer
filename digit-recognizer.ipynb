{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:06:03.245265Z","iopub.execute_input":"2026-01-06T08:06:03.245577Z","iopub.status.idle":"2026-01-06T08:06:03.253409Z","shell.execute_reply.started":"2026-01-06T08:06:03.245555Z","shell.execute_reply":"2026-01-06T08:06:03.251806Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:06:03.255798Z","iopub.execute_input":"2026-01-06T08:06:03.256222Z","iopub.status.idle":"2026-01-06T08:06:03.284225Z","shell.execute_reply.started":"2026-01-06T08:06:03.256189Z","shell.execute_reply":"2026-01-06T08:06:03.282846Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:06:03.285953Z","iopub.execute_input":"2026-01-06T08:06:03.286331Z","iopub.status.idle":"2026-01-06T08:06:03.300830Z","shell.execute_reply.started":"2026-01-06T08:06:03.286302Z","shell.execute_reply":"2026-01-06T08:06:03.299105Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:06:03.302907Z","iopub.execute_input":"2026-01-06T08:06:03.303245Z","iopub.status.idle":"2026-01-06T08:06:03.327393Z","shell.execute_reply.started":"2026-01-06T08:06:03.303219Z","shell.execute_reply":"2026-01-06T08:06:03.325743Z"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split indices for train/validation\ntrain_indices, val_indices = train_test_split(\n    np.arange(len(train_dataset)), test_size=0.2, random_state=42, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:06:03.329209Z","iopub.execute_input":"2026-01-06T08:06:03.329648Z","iopub.status.idle":"2026-01-06T08:06:04.156940Z","shell.execute_reply.started":"2026-01-06T08:06:03.329609Z","shell.execute_reply":"2026-01-06T08:06:04.155809Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class CustomMNISTDataset(Dataset):\n  def __init__(self, csv_file, transform=None, is_test=False):\n    self.data = pd.read_csv(csv_file)\n    self.transform = transform\n    self.is_test = is_test\n\n  def __len__(self):\n    return len(self.data)\n\n  def __getitem__(self, idx):\n    item = self.data.iloc[idx]\n\n    if self.is_test:\n      image = item.values.reshape(28, 28).astype(np.uint8)\n      label = None\n    else:\n      image = item[1:].values.reshape(28, 28).astype(np.uint8)\n      label = item.iloc[0]\n\n    image = transforms.ToPILImage()(image)\n\n    if self.transform is not None:\n      image = self.transform(image)\n\n    if self.is_test:\n      return image\n    else:\n      return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:06:04.157960Z","iopub.execute_input":"2026-01-06T08:06:04.158356Z","iopub.status.idle":"2026-01-06T08:06:04.168728Z","shell.execute_reply.started":"2026-01-06T08:06:04.158333Z","shell.execute_reply":"2026-01-06T08:06:04.167340Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:06:04.170044Z","iopub.execute_input":"2026-01-06T08:06:04.170308Z","iopub.status.idle":"2026-01-06T08:06:04.194568Z","shell.execute_reply.started":"2026-01-06T08:06:04.170284Z","shell.execute_reply":"2026-01-06T08:06:04.193268Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Path to the folder containing the dataset files\npath = \"/kaggle/input/digit-recognizer\"\n\n# List all files in that folder\nfiles = os.listdir(path)\nprint(files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:06:04.195586Z","iopub.execute_input":"2026-01-06T08:06:04.196144Z","iopub.status.idle":"2026-01-06T08:06:04.216111Z","shell.execute_reply.started":"2026-01-06T08:06:04.196109Z","shell.execute_reply":"2026-01-06T08:06:04.215171Z"}},"outputs":[{"name":"stdout","text":"['sample_submission.csv', 'train.csv', 'test.csv']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"train_dataset = CustomMNISTDataset(csv_file=os.path.join(path, 'train.csv'), transform=transform, is_test=False)\ntest_dataset = CustomMNISTDataset(csv_file=os.path.join(path, 'test.csv'), transform=transform, is_test=True)\nprint('Full Train Size: ' + str(len(train_dataset)) +', Test Size: ' + str(len(test_dataset)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:10:31.667341Z","iopub.execute_input":"2026-01-06T08:10:31.667724Z","iopub.status.idle":"2026-01-06T08:10:35.637952Z","shell.execute_reply.started":"2026-01-06T08:10:31.667693Z","shell.execute_reply":"2026-01-06T08:10:35.636928Z"}},"outputs":[{"name":"stdout","text":"Full Train Size: 42000, Test Size: 28000\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:10:35.639517Z","iopub.execute_input":"2026-01-06T08:10:35.639902Z","iopub.status.idle":"2026-01-06T08:10:35.649216Z","shell.execute_reply.started":"2026-01-06T08:10:35.639878Z","shell.execute_reply":"2026-01-06T08:10:35.647968Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"train_dataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:10:35.650187Z","iopub.execute_input":"2026-01-06T08:10:35.650912Z","iopub.status.idle":"2026-01-06T08:10:35.692226Z","shell.execute_reply.started":"2026-01-06T08:10:35.650874Z","shell.execute_reply":"2026-01-06T08:10:35.691139Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000,  0.0745,  0.0745,  0.5059, -0.3255,\n           -0.4353, -0.9922, -0.7490, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -0.8588, -0.7647,  0.9922,  0.9922,  0.9922,  0.7020,\n            0.7020,  0.9294,  0.9922,  0.8118, -0.5765, -0.8824, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -0.8980, -0.3255,  0.9608,  0.9922,  0.9922,  0.9922,  0.9922,\n            0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1843, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n            0.4039,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059, -0.3333,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4353,\n            0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1451, -0.3490,\n            0.5608,  0.9922,  0.5686,  0.9922,  0.9922,  0.9922,  0.3412,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5216,  0.4980,\n            0.9922,  0.9922,  0.9922,  0.9922,  0.1529, -0.6471, -1.0000,\n           -0.9137, -0.7725,  0.0039,  0.9765,  0.9922,  0.9922,  0.6627,\n           -0.4039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3490,  0.9922,\n            0.9922,  0.9922,  0.5843,  0.1529, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000,  0.0039, -0.3490,  0.9922,  0.9922,  0.9922,\n            0.2000,  0.2000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3647,  0.9922,\n            0.9922, -0.3020, -0.4745, -0.4745, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.8824,  0.9922,\n            0.9922,  0.7569, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6314,  0.9922,\n            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3020,  0.9922,\n            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.9922,\n            0.9922,  0.8824, -0.8118, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4118,  0.9922,\n            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.9922,\n            0.9922,  0.4588, -0.9451, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4980,  0.9922,\n            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n            0.9922,  0.9922, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8902,  0.8196,  0.9922,\n            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,  0.6392,\n            0.9922,  0.9922, -0.7569, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8588,  0.9922,  0.2784,\n            0.9922,  0.9922,  0.9922, -0.7725, -0.9059, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -0.8353,  0.2627,  0.9922,\n            0.9922,  0.9216, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843, -0.2627,\n            0.9922,  0.9922,  0.9922,  0.5686, -0.9059, -0.4824, -1.0000,\n           -1.0000, -1.0000, -1.0000, -0.6235,  0.6392,  0.9922,  0.9922,\n            0.9922,  0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8824,\n            0.6157,  0.9922,  0.9922,  0.9922,  0.5843,  0.5216, -0.6235,\n           -0.6235, -0.7333, -0.6784,  0.9922,  0.9922,  0.9922,  0.9922,\n            0.9922, -0.3255, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -0.5294,  0.6627,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n            0.9922,  0.8275,  0.9059,  0.9922,  0.9922,  0.9922,  0.8745,\n           -0.3255, -0.9137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -0.3255,  0.9059,  0.9922,  0.9922,  0.9922,  0.9922,\n            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059, -0.4510,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -0.1059,  0.9922,  0.9922,  0.9922,  0.9922,\n            0.9922,  0.9922,  0.9922,  0.9922,  0.1451, -0.8510, -0.8824,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -0.8980,  0.4275, -0.4039,  0.1451,\n            0.9922,  1.0000,  0.9922,  1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -0.9373, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]),\n 0)"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"batch_size = 64\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:10:37.433496Z","iopub.execute_input":"2026-01-06T08:10:37.433916Z","iopub.status.idle":"2026-01-06T08:10:37.443650Z","shell.execute_reply.started":"2026-01-06T08:10:37.433887Z","shell.execute_reply":"2026-01-06T08:10:37.442034Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"for example_data, example_labels in train_loader:\n  example_image = example_data[0]\n  print(\"Input Size:\", example_data.size())\n\n  example_image_numpy = example_image.permute(1, 2, 0).numpy()\n\n  plt.imshow(example_image_numpy)\n  plt.title(f'Label: {example_labels[0]}')\n  plt.show()\n\n  break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:10:40.983610Z","iopub.execute_input":"2026-01-06T08:10:40.984059Z","iopub.status.idle":"2026-01-06T08:10:41.780241Z","shell.execute_reply.started":"2026-01-06T08:10:40.984030Z","shell.execute_reply":"2026-01-06T08:10:41.779154Z"}},"outputs":[{"name":"stdout","text":"Input Size: torch.Size([64, 1, 28, 28])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        \n        # Flattened size after conv + pool: 128*7*7\n        self.fc1 = nn.Linear(128 * 7 * 7, 128)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, 10)  # 10 classes for MNIST\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pool(x)\n        \n        x = x.view(x.size(0), -1)  # flatten except batch dimension\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:10:44.323784Z","iopub.execute_input":"2026-01-06T08:10:44.324206Z","iopub.status.idle":"2026-01-06T08:10:44.335081Z","shell.execute_reply.started":"2026-01-06T08:10:44.324178Z","shell.execute_reply":"2026-01-06T08:10:44.333830Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"model = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:10:47.005226Z","iopub.execute_input":"2026-01-06T08:10:47.005591Z","iopub.status.idle":"2026-01-06T08:10:47.026187Z","shell.execute_reply.started":"2026-01-06T08:10:47.005569Z","shell.execute_reply":"2026-01-06T08:10:47.024854Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"num_epochs = 100\nrunning_loss = 0.0\n\npatience = 10      \nbest_val_loss = float('inf')\ntrigger_times = 0\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs.float())\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 100 == 99:\n            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100}')\n            running_loss = 0.0\n\n    # --- Validation step ---\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs.float())\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n    val_loss /= len(val_loader)\n    print(f'Epoch {epoch+1} Validation Loss: {val_loss:.4f}')\n\n    # --- Early stopping check ---\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        trigger_times = 0\n    else:\n        trigger_times += 1\n        if trigger_times >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T08:10:48.862113Z","iopub.execute_input":"2026-01-06T08:10:48.862489Z","iopub.status.idle":"2026-01-06T10:08:37.281643Z","shell.execute_reply.started":"2026-01-06T08:10:48.862462Z","shell.execute_reply":"2026-01-06T10:08:37.280421Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1, Batch: 100, Loss: 0.8049618650972843\nEpoch: 1, Batch: 200, Loss: 0.26000945426523686\nEpoch: 1, Batch: 300, Loss: 0.21197907775640487\nEpoch: 1, Batch: 400, Loss: 0.1615475196391344\nEpoch: 1, Batch: 500, Loss: 0.17333012856543065\nEpoch: 1, Batch: 600, Loss: 0.14830302538350223\nEpoch 1 Validation Loss: 0.0674\nEpoch: 2, Batch: 100, Loss: 0.11254524478688836\nEpoch: 2, Batch: 200, Loss: 0.1233158627524972\nEpoch: 2, Batch: 300, Loss: 0.10695932413451374\nEpoch: 2, Batch: 400, Loss: 0.10062926536425948\nEpoch: 2, Batch: 500, Loss: 0.10382881655357779\nEpoch: 2, Batch: 600, Loss: 0.10263962538912892\nEpoch 2 Validation Loss: 0.0475\nEpoch: 3, Batch: 100, Loss: 0.08701369190239347\nEpoch: 3, Batch: 200, Loss: 0.07501049472484737\nEpoch: 3, Batch: 300, Loss: 0.07564542716369033\nEpoch: 3, Batch: 400, Loss: 0.08587569449096918\nEpoch: 3, Batch: 500, Loss: 0.08508513727225363\nEpoch: 3, Batch: 600, Loss: 0.08738437296822667\nEpoch 3 Validation Loss: 0.0431\nEpoch: 4, Batch: 100, Loss: 0.07202134842984378\nEpoch: 4, Batch: 200, Loss: 0.07011107015190646\nEpoch: 4, Batch: 300, Loss: 0.06374260436743498\nEpoch: 4, Batch: 400, Loss: 0.06343430652748794\nEpoch: 4, Batch: 500, Loss: 0.06596517328172923\nEpoch: 4, Batch: 600, Loss: 0.07509685939177871\nEpoch 4 Validation Loss: 0.0311\nEpoch: 5, Batch: 100, Loss: 0.055410259310156106\nEpoch: 5, Batch: 200, Loss: 0.060514349546283486\nEpoch: 5, Batch: 300, Loss: 0.058120625275187195\nEpoch: 5, Batch: 400, Loss: 0.057153528218623253\nEpoch: 5, Batch: 500, Loss: 0.0644508856907487\nEpoch: 5, Batch: 600, Loss: 0.06675111720804125\nEpoch 5 Validation Loss: 0.0307\nEpoch: 6, Batch: 100, Loss: 0.057174363100202755\nEpoch: 6, Batch: 200, Loss: 0.0474309687060304\nEpoch: 6, Batch: 300, Loss: 0.05581102450843901\nEpoch: 6, Batch: 400, Loss: 0.05300791366724297\nEpoch: 6, Batch: 500, Loss: 0.050591969296801834\nEpoch: 6, Batch: 600, Loss: 0.05578266651602462\nEpoch 6 Validation Loss: 0.0296\nEpoch: 7, Batch: 100, Loss: 0.0384974417113699\nEpoch: 7, Batch: 200, Loss: 0.0532997213723138\nEpoch: 7, Batch: 300, Loss: 0.03953340977896005\nEpoch: 7, Batch: 400, Loss: 0.059573493991047145\nEpoch: 7, Batch: 500, Loss: 0.04382210803800263\nEpoch: 7, Batch: 600, Loss: 0.04643715488258749\nEpoch 7 Validation Loss: 0.0213\nEpoch: 8, Batch: 100, Loss: 0.04500396176474169\nEpoch: 8, Batch: 200, Loss: 0.04706512409262359\nEpoch: 8, Batch: 300, Loss: 0.04992025971994735\nEpoch: 8, Batch: 400, Loss: 0.042166581470519306\nEpoch: 8, Batch: 500, Loss: 0.04805108896340243\nEpoch: 8, Batch: 600, Loss: 0.05075770948315039\nEpoch 8 Validation Loss: 0.0172\nEpoch: 9, Batch: 100, Loss: 0.04150614804355428\nEpoch: 9, Batch: 200, Loss: 0.03737572297628503\nEpoch: 9, Batch: 300, Loss: 0.04724909484968521\nEpoch: 9, Batch: 400, Loss: 0.038297108099795876\nEpoch: 9, Batch: 500, Loss: 0.03696978274936555\nEpoch: 9, Batch: 600, Loss: 0.05223324951482937\nEpoch 9 Validation Loss: 0.0153\nEpoch: 10, Batch: 100, Loss: 0.03433441295695957\nEpoch: 10, Batch: 200, Loss: 0.03917309892596677\nEpoch: 10, Batch: 300, Loss: 0.04221400270762388\nEpoch: 10, Batch: 400, Loss: 0.03175451970251743\nEpoch: 10, Batch: 500, Loss: 0.03676400242198724\nEpoch: 10, Batch: 600, Loss: 0.03940265347744571\nEpoch 10 Validation Loss: 0.0140\nEpoch: 11, Batch: 100, Loss: 0.03748585102555808\nEpoch: 11, Batch: 200, Loss: 0.03443785460258368\nEpoch: 11, Batch: 300, Loss: 0.041335457799141294\nEpoch: 11, Batch: 400, Loss: 0.03352996331872418\nEpoch: 11, Batch: 500, Loss: 0.037597630745731296\nEpoch: 11, Batch: 600, Loss: 0.04327566239517182\nEpoch 11 Validation Loss: 0.0105\nEpoch: 12, Batch: 100, Loss: 0.03824959239442251\nEpoch: 12, Batch: 200, Loss: 0.036865348832216115\nEpoch: 12, Batch: 300, Loss: 0.029796503309044056\nEpoch: 12, Batch: 400, Loss: 0.03091553685575491\nEpoch: 12, Batch: 500, Loss: 0.0284959757613251\nEpoch: 12, Batch: 600, Loss: 0.035115656458074226\nEpoch 12 Validation Loss: 0.0095\nEpoch: 13, Batch: 100, Loss: 0.02785007493570447\nEpoch: 13, Batch: 200, Loss: 0.03519712583569344\nEpoch: 13, Batch: 300, Loss: 0.027479779224668165\nEpoch: 13, Batch: 400, Loss: 0.03389116484235274\nEpoch: 13, Batch: 500, Loss: 0.03314512338867644\nEpoch: 13, Batch: 600, Loss: 0.032859086844546255\nEpoch 13 Validation Loss: 0.0114\nEpoch: 14, Batch: 100, Loss: 0.03200628668040736\nEpoch: 14, Batch: 200, Loss: 0.03245346754963976\nEpoch: 14, Batch: 300, Loss: 0.025970395499607547\nEpoch: 14, Batch: 400, Loss: 0.03361128689837642\nEpoch: 14, Batch: 500, Loss: 0.04150576850748621\nEpoch: 14, Batch: 600, Loss: 0.032436283297720366\nEpoch 14 Validation Loss: 0.0105\nEpoch: 15, Batch: 100, Loss: 0.031719474244164304\nEpoch: 15, Batch: 200, Loss: 0.038096873827162196\nEpoch: 15, Batch: 300, Loss: 0.03275435127034143\nEpoch: 15, Batch: 400, Loss: 0.02995886016113218\nEpoch: 15, Batch: 500, Loss: 0.029382431312697008\nEpoch: 15, Batch: 600, Loss: 0.020712960441887845\nEpoch 15 Validation Loss: 0.0126\nEpoch: 16, Batch: 100, Loss: 0.02062140286900103\nEpoch: 16, Batch: 200, Loss: 0.02866865525516914\nEpoch: 16, Batch: 300, Loss: 0.03147960964561207\nEpoch: 16, Batch: 400, Loss: 0.026629234553547577\nEpoch: 16, Batch: 500, Loss: 0.030790484911121893\nEpoch: 16, Batch: 600, Loss: 0.03037522646365687\nEpoch 16 Validation Loss: 0.0087\nEpoch: 17, Batch: 100, Loss: 0.025902612750069238\nEpoch: 17, Batch: 200, Loss: 0.019907408886356278\nEpoch: 17, Batch: 300, Loss: 0.023645652388513554\nEpoch: 17, Batch: 400, Loss: 0.02025947388712666\nEpoch: 17, Batch: 500, Loss: 0.024750631721763058\nEpoch: 17, Batch: 600, Loss: 0.03316514365316834\nEpoch 17 Validation Loss: 0.0096\nEpoch: 18, Batch: 100, Loss: 0.020127176117239288\nEpoch: 18, Batch: 200, Loss: 0.02109150255440909\nEpoch: 18, Batch: 300, Loss: 0.02264729071763213\nEpoch: 18, Batch: 400, Loss: 0.02606810025390587\nEpoch: 18, Batch: 500, Loss: 0.027358602647145746\nEpoch: 18, Batch: 600, Loss: 0.02902431279886514\nEpoch 18 Validation Loss: 0.0116\nEpoch: 19, Batch: 100, Loss: 0.02157961429518764\nEpoch: 19, Batch: 200, Loss: 0.02919825714372564\nEpoch: 19, Batch: 300, Loss: 0.027736497105215675\nEpoch: 19, Batch: 400, Loss: 0.0251285222088336\nEpoch: 19, Batch: 500, Loss: 0.021458757871732813\nEpoch: 19, Batch: 600, Loss: 0.02296744547027629\nEpoch 19 Validation Loss: 0.0132\nEpoch: 20, Batch: 100, Loss: 0.02163844078211696\nEpoch: 20, Batch: 200, Loss: 0.017803290127485525\nEpoch: 20, Batch: 300, Loss: 0.021918277654913255\nEpoch: 20, Batch: 400, Loss: 0.02679537544026971\nEpoch: 20, Batch: 500, Loss: 0.02998784021503525\nEpoch: 20, Batch: 600, Loss: 0.026015350205561845\nEpoch 20 Validation Loss: 0.0074\nEpoch: 21, Batch: 100, Loss: 0.01975891056164983\nEpoch: 21, Batch: 200, Loss: 0.019747664435817568\nEpoch: 21, Batch: 300, Loss: 0.019923931484081548\nEpoch: 21, Batch: 400, Loss: 0.0309107725310605\nEpoch: 21, Batch: 500, Loss: 0.028901360227609984\nEpoch: 21, Batch: 600, Loss: 0.022882306538376723\nEpoch 21 Validation Loss: 0.0082\nEpoch: 22, Batch: 100, Loss: 0.013915321796557691\nEpoch: 22, Batch: 200, Loss: 0.02429461184685351\nEpoch: 22, Batch: 300, Loss: 0.022602158848603723\nEpoch: 22, Batch: 400, Loss: 0.026332350406155457\nEpoch: 22, Batch: 500, Loss: 0.018719307367209694\nEpoch: 22, Batch: 600, Loss: 0.028601557281363058\nEpoch 22 Validation Loss: 0.0080\nEpoch: 23, Batch: 100, Loss: 0.020223581816680963\nEpoch: 23, Batch: 200, Loss: 0.016676344648003577\nEpoch: 23, Batch: 300, Loss: 0.024040855811035725\nEpoch: 23, Batch: 400, Loss: 0.023286745558289114\nEpoch: 23, Batch: 500, Loss: 0.02004673676514358\nEpoch: 23, Batch: 600, Loss: 0.027913355479831807\nEpoch 23 Validation Loss: 0.0092\nEpoch: 24, Batch: 100, Loss: 0.027974966956244317\nEpoch: 24, Batch: 200, Loss: 0.017612076344084925\nEpoch: 24, Batch: 300, Loss: 0.020464820235793012\nEpoch: 24, Batch: 400, Loss: 0.019250369145011062\nEpoch: 24, Batch: 500, Loss: 0.022772651166669676\nEpoch: 24, Batch: 600, Loss: 0.0216154061704583\nEpoch 24 Validation Loss: 0.0065\nEpoch: 25, Batch: 100, Loss: 0.015988583140424454\nEpoch: 25, Batch: 200, Loss: 0.016345602866786067\nEpoch: 25, Batch: 300, Loss: 0.02842656480403093\nEpoch: 25, Batch: 400, Loss: 0.019526823495398277\nEpoch: 25, Batch: 500, Loss: 0.02050374325568555\nEpoch: 25, Batch: 600, Loss: 0.022368646321701817\nEpoch 25 Validation Loss: 0.0049\nEpoch: 26, Batch: 100, Loss: 0.01707840837429103\nEpoch: 26, Batch: 200, Loss: 0.023485647950456042\nEpoch: 26, Batch: 300, Loss: 0.02596233979158569\nEpoch: 26, Batch: 400, Loss: 0.018685159273154568\nEpoch: 26, Batch: 500, Loss: 0.021995641108806013\nEpoch: 26, Batch: 600, Loss: 0.019843801962288125\nEpoch 26 Validation Loss: 0.0071\nEpoch: 27, Batch: 100, Loss: 0.019547992853877077\nEpoch: 27, Batch: 200, Loss: 0.018929333063133526\nEpoch: 27, Batch: 300, Loss: 0.02145989832319174\nEpoch: 27, Batch: 400, Loss: 0.01527682560899848\nEpoch: 27, Batch: 500, Loss: 0.014840868334504194\nEpoch: 27, Batch: 600, Loss: 0.020562648355480634\nEpoch 27 Validation Loss: 0.0067\nEpoch: 28, Batch: 100, Loss: 0.021476589767644327\nEpoch: 28, Batch: 200, Loss: 0.018338761139493728\nEpoch: 28, Batch: 300, Loss: 0.019951159617430676\nEpoch: 28, Batch: 400, Loss: 0.014699077758559725\nEpoch: 28, Batch: 500, Loss: 0.025279405874316582\nEpoch: 28, Batch: 600, Loss: 0.016132745463764878\nEpoch 28 Validation Loss: 0.0058\nEpoch: 29, Batch: 100, Loss: 0.011313149697962217\nEpoch: 29, Batch: 200, Loss: 0.02455753642494528\nEpoch: 29, Batch: 300, Loss: 0.026315492989997438\nEpoch: 29, Batch: 400, Loss: 0.015737629327468312\nEpoch: 29, Batch: 500, Loss: 0.021946230373141588\nEpoch: 29, Batch: 600, Loss: 0.022313772910274567\nEpoch 29 Validation Loss: 0.0101\nEpoch: 30, Batch: 100, Loss: 0.02064554382326605\nEpoch: 30, Batch: 200, Loss: 0.01442449231595674\nEpoch: 30, Batch: 300, Loss: 0.01555347217252347\nEpoch: 30, Batch: 400, Loss: 0.020444211094763888\nEpoch: 30, Batch: 500, Loss: 0.021603677548628183\nEpoch: 30, Batch: 600, Loss: 0.024097814055057825\nEpoch 30 Validation Loss: 0.0075\nEpoch: 31, Batch: 100, Loss: 0.01543566349089815\nEpoch: 31, Batch: 200, Loss: 0.012663816565800517\nEpoch: 31, Batch: 300, Loss: 0.017227996415667805\nEpoch: 31, Batch: 400, Loss: 0.014944939160996\nEpoch: 31, Batch: 500, Loss: 0.017252897426660637\nEpoch: 31, Batch: 600, Loss: 0.020694631889091396\nEpoch 31 Validation Loss: 0.0071\nEpoch: 32, Batch: 100, Loss: 0.017706410914761363\nEpoch: 32, Batch: 200, Loss: 0.022265079767494173\nEpoch: 32, Batch: 300, Loss: 0.019215180334285833\nEpoch: 32, Batch: 400, Loss: 0.014398668188387092\nEpoch: 32, Batch: 500, Loss: 0.03051114205505655\nEpoch: 32, Batch: 600, Loss: 0.019447173850494437\nEpoch 32 Validation Loss: 0.0054\nEpoch: 33, Batch: 100, Loss: 0.018440303763018164\nEpoch: 33, Batch: 200, Loss: 0.021340543350706866\nEpoch: 33, Batch: 300, Loss: 0.013641772960763774\nEpoch: 33, Batch: 400, Loss: 0.014016221660276641\nEpoch: 33, Batch: 500, Loss: 0.01701705304558345\nEpoch: 33, Batch: 600, Loss: 0.012371791648765793\nEpoch 33 Validation Loss: 0.0043\nEpoch: 34, Batch: 100, Loss: 0.01277398587168136\nEpoch: 34, Batch: 200, Loss: 0.015986907033639\nEpoch: 34, Batch: 300, Loss: 0.01793201481275901\nEpoch: 34, Batch: 400, Loss: 0.014213338851013758\nEpoch: 34, Batch: 500, Loss: 0.02317144998800359\nEpoch: 34, Batch: 600, Loss: 0.01224213188943395\nEpoch 34 Validation Loss: 0.0043\nEpoch: 35, Batch: 100, Loss: 0.01380273381623283\nEpoch: 35, Batch: 200, Loss: 0.01765585463523166\nEpoch: 35, Batch: 300, Loss: 0.015278581233287696\nEpoch: 35, Batch: 400, Loss: 0.021387587804711075\nEpoch: 35, Batch: 500, Loss: 0.01320349928179894\nEpoch: 35, Batch: 600, Loss: 0.02115847768920503\nEpoch 35 Validation Loss: 0.0073\nEpoch: 36, Batch: 100, Loss: 0.013305077309942135\nEpoch: 36, Batch: 200, Loss: 0.01740419934883903\nEpoch: 36, Batch: 300, Loss: 0.015602550802632321\nEpoch: 36, Batch: 400, Loss: 0.0206170827706228\nEpoch: 36, Batch: 500, Loss: 0.01791499976265186\nEpoch: 36, Batch: 600, Loss: 0.014423035579275165\nEpoch 36 Validation Loss: 0.0083\nEpoch: 37, Batch: 100, Loss: 0.013939005556349002\nEpoch: 37, Batch: 200, Loss: 0.012090417018371227\nEpoch: 37, Batch: 300, Loss: 0.014334513942321792\nEpoch: 37, Batch: 400, Loss: 0.017015600940994774\nEpoch: 37, Batch: 500, Loss: 0.012451407793269026\nEpoch: 37, Batch: 600, Loss: 0.012080019188397273\nEpoch 37 Validation Loss: 0.0071\nEpoch: 38, Batch: 100, Loss: 0.012479960345567634\nEpoch: 38, Batch: 200, Loss: 0.017254642427396903\nEpoch: 38, Batch: 300, Loss: 0.016939896252952166\nEpoch: 38, Batch: 400, Loss: 0.013384304614965004\nEpoch: 38, Batch: 500, Loss: 0.015909249811411428\nEpoch: 38, Batch: 600, Loss: 0.012607111016686759\nEpoch 38 Validation Loss: 0.0035\nEpoch: 39, Batch: 100, Loss: 0.009685267560216744\nEpoch: 39, Batch: 200, Loss: 0.00922907703033161\nEpoch: 39, Batch: 300, Loss: 0.014154596415983179\nEpoch: 39, Batch: 400, Loss: 0.019387922896964938\nEpoch: 39, Batch: 500, Loss: 0.01605254770285683\nEpoch: 39, Batch: 600, Loss: 0.024270289616906665\nEpoch 39 Validation Loss: 0.0051\nEpoch: 40, Batch: 100, Loss: 0.015013987373240525\nEpoch: 40, Batch: 200, Loss: 0.012697785949799254\nEpoch: 40, Batch: 300, Loss: 0.02136373254341379\nEpoch: 40, Batch: 400, Loss: 0.021893764440792437\nEpoch: 40, Batch: 500, Loss: 0.015417741948840557\nEpoch: 40, Batch: 600, Loss: 0.02762945879054314\nEpoch 40 Validation Loss: 0.0034\nEpoch: 41, Batch: 100, Loss: 0.014253749446907023\nEpoch: 41, Batch: 200, Loss: 0.014924297126799501\nEpoch: 41, Batch: 300, Loss: 0.013869579191336925\nEpoch: 41, Batch: 400, Loss: 0.019834005341531336\nEpoch: 41, Batch: 500, Loss: 0.014391955822902673\nEpoch: 41, Batch: 600, Loss: 0.020253769772825763\nEpoch 41 Validation Loss: 0.0039\nEpoch: 42, Batch: 100, Loss: 0.01506903334884555\nEpoch: 42, Batch: 200, Loss: 0.012039332102194749\nEpoch: 42, Batch: 300, Loss: 0.01660248209270776\nEpoch: 42, Batch: 400, Loss: 0.016761082119173806\nEpoch: 42, Batch: 500, Loss: 0.012573678438184289\nEpoch: 42, Batch: 600, Loss: 0.021041616614384112\nEpoch 42 Validation Loss: 0.0038\nEpoch: 43, Batch: 100, Loss: 0.01589809863253322\nEpoch: 43, Batch: 200, Loss: 0.012397000231612765\nEpoch: 43, Batch: 300, Loss: 0.014535134207399096\nEpoch: 43, Batch: 400, Loss: 0.014333026493004582\nEpoch: 43, Batch: 500, Loss: 0.0055910314309949175\nEpoch: 43, Batch: 600, Loss: 0.020546508948109478\nEpoch 43 Validation Loss: 0.0090\nEpoch: 44, Batch: 100, Loss: 0.018059169794760237\nEpoch: 44, Batch: 200, Loss: 0.013936143123682996\nEpoch: 44, Batch: 300, Loss: 0.013817093781290169\nEpoch: 44, Batch: 400, Loss: 0.015409722134954791\nEpoch: 44, Batch: 500, Loss: 0.011858486692501629\nEpoch: 44, Batch: 600, Loss: 0.015329378537135199\nEpoch 44 Validation Loss: 0.0076\nEpoch: 45, Batch: 100, Loss: 0.009417844643880926\nEpoch: 45, Batch: 200, Loss: 0.010324600027915949\nEpoch: 45, Batch: 300, Loss: 0.013432324316818267\nEpoch: 45, Batch: 400, Loss: 0.010651584066163195\nEpoch: 45, Batch: 500, Loss: 0.009221330354284874\nEpoch: 45, Batch: 600, Loss: 0.01583193101444152\nEpoch 45 Validation Loss: 0.0048\nEpoch: 46, Batch: 100, Loss: 0.014547830811425229\nEpoch: 46, Batch: 200, Loss: 0.012826673386364291\nEpoch: 46, Batch: 300, Loss: 0.0188098768841337\nEpoch: 46, Batch: 400, Loss: 0.01649187827753849\nEpoch: 46, Batch: 500, Loss: 0.011527475818411403\nEpoch: 46, Batch: 600, Loss: 0.013649643480603118\nEpoch 46 Validation Loss: 0.0029\nEpoch: 47, Batch: 100, Loss: 0.013863512833695494\nEpoch: 47, Batch: 200, Loss: 0.01435840405516501\nEpoch: 47, Batch: 300, Loss: 0.011602239828462188\nEpoch: 47, Batch: 400, Loss: 0.013822088381766661\nEpoch: 47, Batch: 500, Loss: 0.009459044622681177\nEpoch: 47, Batch: 600, Loss: 0.018611731934261116\nEpoch 47 Validation Loss: 0.0037\nEpoch: 48, Batch: 100, Loss: 0.017315192013265913\nEpoch: 48, Batch: 200, Loss: 0.01716654841066884\nEpoch: 48, Batch: 300, Loss: 0.02067145094195439\nEpoch: 48, Batch: 400, Loss: 0.01613496435013076\nEpoch: 48, Batch: 500, Loss: 0.016093858761669255\nEpoch: 48, Batch: 600, Loss: 0.010922360550539452\nEpoch 48 Validation Loss: 0.0027\nEpoch: 49, Batch: 100, Loss: 0.012770080275367945\nEpoch: 49, Batch: 200, Loss: 0.014967010391687836\nEpoch: 49, Batch: 300, Loss: 0.013395441648222005\nEpoch: 49, Batch: 400, Loss: 0.014782280058952893\nEpoch: 49, Batch: 500, Loss: 0.01928893668137789\nEpoch: 49, Batch: 600, Loss: 0.014019303187524201\nEpoch 49 Validation Loss: 0.0022\nEpoch: 50, Batch: 100, Loss: 0.013113943412051868\nEpoch: 50, Batch: 200, Loss: 0.01326665914863952\nEpoch: 50, Batch: 300, Loss: 0.011978078908814496\nEpoch: 50, Batch: 400, Loss: 0.009665754861089226\nEpoch: 50, Batch: 500, Loss: 0.017861364432319532\nEpoch: 50, Batch: 600, Loss: 0.01251448831369089\nEpoch 50 Validation Loss: 0.0048\nEpoch: 51, Batch: 100, Loss: 0.011624485819556867\nEpoch: 51, Batch: 200, Loss: 0.01338261428041733\nEpoch: 51, Batch: 300, Loss: 0.012202523328360257\nEpoch: 51, Batch: 400, Loss: 0.012400785568770516\nEpoch: 51, Batch: 500, Loss: 0.01305874730863252\nEpoch: 51, Batch: 600, Loss: 0.01679091539243018\nEpoch 51 Validation Loss: 0.0025\nEpoch: 52, Batch: 100, Loss: 0.015506725503801135\nEpoch: 52, Batch: 200, Loss: 0.010292217744977279\nEpoch: 52, Batch: 300, Loss: 0.013797747668045304\nEpoch: 52, Batch: 400, Loss: 0.00873055949969512\nEpoch: 52, Batch: 500, Loss: 0.017125882063792234\nEpoch: 52, Batch: 600, Loss: 0.013588219354660395\nEpoch 52 Validation Loss: 0.0028\nEpoch: 53, Batch: 100, Loss: 0.012303346236167271\nEpoch: 53, Batch: 200, Loss: 0.012580172832226708\nEpoch: 53, Batch: 300, Loss: 0.009177531582192842\nEpoch: 53, Batch: 400, Loss: 0.017175213805387557\nEpoch: 53, Batch: 500, Loss: 0.018497277701189887\nEpoch: 53, Batch: 600, Loss: 0.010233249089424135\nEpoch 53 Validation Loss: 0.0039\nEpoch: 54, Batch: 100, Loss: 0.010814354680496763\nEpoch: 54, Batch: 200, Loss: 0.009869817406074616\nEpoch: 54, Batch: 300, Loss: 0.018219129229364625\nEpoch: 54, Batch: 400, Loss: 0.015619059943583124\nEpoch: 54, Batch: 500, Loss: 0.008521588605981378\nEpoch: 54, Batch: 600, Loss: 0.010311000108631561\nEpoch 54 Validation Loss: 0.0036\nEpoch: 55, Batch: 100, Loss: 0.013322239967756104\nEpoch: 55, Batch: 200, Loss: 0.01518659231264337\nEpoch: 55, Batch: 300, Loss: 0.010581575620622062\nEpoch: 55, Batch: 400, Loss: 0.008199916426583513\nEpoch: 55, Batch: 500, Loss: 0.008428836121149743\nEpoch: 55, Batch: 600, Loss: 0.013746495830362164\nEpoch 55 Validation Loss: 0.0027\nEpoch: 56, Batch: 100, Loss: 0.014512546134101285\nEpoch: 56, Batch: 200, Loss: 0.013465592237437249\nEpoch: 56, Batch: 300, Loss: 0.017469460410484316\nEpoch: 56, Batch: 400, Loss: 0.014661495982363704\nEpoch: 56, Batch: 500, Loss: 0.015456939372179478\nEpoch: 56, Batch: 600, Loss: 0.010200079731500295\nEpoch 56 Validation Loss: 0.0040\nEpoch: 57, Batch: 100, Loss: 0.016976065836715862\nEpoch: 57, Batch: 200, Loss: 0.015217607573140413\nEpoch: 57, Batch: 300, Loss: 0.011578993607281518\nEpoch: 57, Batch: 400, Loss: 0.012849621078248673\nEpoch: 57, Batch: 500, Loss: 0.01127253979224406\nEpoch: 57, Batch: 600, Loss: 0.013414486734782259\nEpoch 57 Validation Loss: 0.0024\nEpoch: 58, Batch: 100, Loss: 0.010634802329430072\nEpoch: 58, Batch: 200, Loss: 0.006057553595364879\nEpoch: 58, Batch: 300, Loss: 0.01385615784658512\nEpoch: 58, Batch: 400, Loss: 0.010272594180469241\nEpoch: 58, Batch: 500, Loss: 0.010909552383868117\nEpoch: 58, Batch: 600, Loss: 0.010461320132021683\nEpoch 58 Validation Loss: 0.0051\nEpoch: 59, Batch: 100, Loss: 0.011741473711318805\nEpoch: 59, Batch: 200, Loss: 0.01636596281575862\nEpoch: 59, Batch: 300, Loss: 0.016578749756445178\nEpoch: 59, Batch: 400, Loss: 0.010195567423843387\nEpoch: 59, Batch: 500, Loss: 0.010416995759549082\nEpoch: 59, Batch: 600, Loss: 0.01080134123519656\nEpoch 59 Validation Loss: 0.0015\nEpoch: 60, Batch: 100, Loss: 0.011102858056220839\nEpoch: 60, Batch: 200, Loss: 0.01295032622469762\nEpoch: 60, Batch: 300, Loss: 0.011986349720399404\nEpoch: 60, Batch: 400, Loss: 0.013339204050571424\nEpoch: 60, Batch: 500, Loss: 0.012372618015924673\nEpoch: 60, Batch: 600, Loss: 0.015806679100514885\nEpoch 60 Validation Loss: 0.0059\nEpoch: 61, Batch: 100, Loss: 0.015684027302504545\nEpoch: 61, Batch: 200, Loss: 0.0209376099270321\nEpoch: 61, Batch: 300, Loss: 0.015543898644768888\nEpoch: 61, Batch: 400, Loss: 0.012356948256419855\nEpoch: 61, Batch: 500, Loss: 0.018254828000099223\nEpoch: 61, Batch: 600, Loss: 0.012937677723803062\nEpoch 61 Validation Loss: 0.0022\nEpoch: 62, Batch: 100, Loss: 0.008961205472514848\nEpoch: 62, Batch: 200, Loss: 0.015586094610443978\nEpoch: 62, Batch: 300, Loss: 0.013102481645837542\nEpoch: 62, Batch: 400, Loss: 0.00830381975948967\nEpoch: 62, Batch: 500, Loss: 0.013105725985788012\nEpoch: 62, Batch: 600, Loss: 0.011929860975678821\nEpoch 62 Validation Loss: 0.0024\nEpoch: 63, Batch: 100, Loss: 0.012189296054448278\nEpoch: 63, Batch: 200, Loss: 0.010391819387990608\nEpoch: 63, Batch: 300, Loss: 0.01618423587576217\nEpoch: 63, Batch: 400, Loss: 0.010080936250460582\nEpoch: 63, Batch: 500, Loss: 0.013257569446559501\nEpoch: 63, Batch: 600, Loss: 0.00813588135799364\nEpoch 63 Validation Loss: 0.0051\nEpoch: 64, Batch: 100, Loss: 0.018382674197041524\nEpoch: 64, Batch: 200, Loss: 0.009276719787185356\nEpoch: 64, Batch: 300, Loss: 0.01020840666281856\nEpoch: 64, Batch: 400, Loss: 0.007747618828528857\nEpoch: 64, Batch: 500, Loss: 0.009048235798763926\nEpoch: 64, Batch: 600, Loss: 0.01424316884679456\nEpoch 64 Validation Loss: 0.0030\nEpoch: 65, Batch: 100, Loss: 0.008186029139151287\nEpoch: 65, Batch: 200, Loss: 0.012986594973212959\nEpoch: 65, Batch: 300, Loss: 0.014948990535212942\nEpoch: 65, Batch: 400, Loss: 0.01258654420212224\nEpoch: 65, Batch: 500, Loss: 0.01598168627152063\nEpoch: 65, Batch: 600, Loss: 0.01690272332632958\nEpoch 65 Validation Loss: 0.0030\nEpoch: 66, Batch: 100, Loss: 0.00899185916616716\nEpoch: 66, Batch: 200, Loss: 0.010745508018409282\nEpoch: 66, Batch: 300, Loss: 0.013046483560419802\nEpoch: 66, Batch: 400, Loss: 0.01489685800675943\nEpoch: 66, Batch: 500, Loss: 0.0128753424444119\nEpoch: 66, Batch: 600, Loss: 0.013708150636265088\nEpoch 66 Validation Loss: 0.0026\nEpoch: 67, Batch: 100, Loss: 0.013140411890973383\nEpoch: 67, Batch: 200, Loss: 0.011736399626520323\nEpoch: 67, Batch: 300, Loss: 0.014944518354432147\nEpoch: 67, Batch: 400, Loss: 0.009887596777078898\nEpoch: 67, Batch: 500, Loss: 0.01377938389132396\nEpoch: 67, Batch: 600, Loss: 0.010989413657748628\nEpoch 67 Validation Loss: 0.0025\nEpoch: 68, Batch: 100, Loss: 0.011769567452122374\nEpoch: 68, Batch: 200, Loss: 0.009506321899971226\nEpoch: 68, Batch: 300, Loss: 0.014189884244240147\nEpoch: 68, Batch: 400, Loss: 0.014176112818940965\nEpoch: 68, Batch: 500, Loss: 0.012698734004516155\nEpoch: 68, Batch: 600, Loss: 0.012303058423567563\nEpoch 68 Validation Loss: 0.0024\nEpoch: 69, Batch: 100, Loss: 0.009652906719203429\nEpoch: 69, Batch: 200, Loss: 0.007908619002942032\nEpoch: 69, Batch: 300, Loss: 0.011091854452942513\nEpoch: 69, Batch: 400, Loss: 0.009265351468307016\nEpoch: 69, Batch: 500, Loss: 0.008415804517788956\nEpoch: 69, Batch: 600, Loss: 0.01165029900342347\nEpoch 69 Validation Loss: 0.0011\nEpoch: 70, Batch: 100, Loss: 0.00789344392586429\nEpoch: 70, Batch: 200, Loss: 0.011291585473304621\nEpoch: 70, Batch: 300, Loss: 0.013843702528183711\nEpoch: 70, Batch: 400, Loss: 0.017176669566271698\nEpoch: 70, Batch: 500, Loss: 0.01295563171379996\nEpoch: 70, Batch: 600, Loss: 0.017095095827648947\nEpoch 70 Validation Loss: 0.0009\nEpoch: 71, Batch: 100, Loss: 0.010337509309169945\nEpoch: 71, Batch: 200, Loss: 0.01677577511237132\nEpoch: 71, Batch: 300, Loss: 0.0106947596912687\nEpoch: 71, Batch: 400, Loss: 0.01429064076875875\nEpoch: 71, Batch: 500, Loss: 0.012512940143378727\nEpoch: 71, Batch: 600, Loss: 0.010670173143973899\nEpoch 71 Validation Loss: 0.0013\nEpoch: 72, Batch: 100, Loss: 0.006837917284390187\nEpoch: 72, Batch: 200, Loss: 0.010589701400386958\nEpoch: 72, Batch: 300, Loss: 0.012377412132941572\nEpoch: 72, Batch: 400, Loss: 0.011076977980092124\nEpoch: 72, Batch: 500, Loss: 0.008202659671695187\nEpoch: 72, Batch: 600, Loss: 0.012693735998069541\nEpoch 72 Validation Loss: 0.0026\nEpoch: 73, Batch: 100, Loss: 0.009403067860926057\nEpoch: 73, Batch: 200, Loss: 0.006950494535872167\nEpoch: 73, Batch: 300, Loss: 0.009277602458536762\nEpoch: 73, Batch: 400, Loss: 0.01407021341889049\nEpoch: 73, Batch: 500, Loss: 0.013455711020342279\nEpoch: 73, Batch: 600, Loss: 0.01998046526154667\nEpoch 73 Validation Loss: 0.0049\nEpoch: 74, Batch: 100, Loss: 0.014957982262949372\nEpoch: 74, Batch: 200, Loss: 0.015849835438366426\nEpoch: 74, Batch: 300, Loss: 0.011014546106034686\nEpoch: 74, Batch: 400, Loss: 0.008622104963105812\nEpoch: 74, Batch: 500, Loss: 0.018391043273149988\nEpoch: 74, Batch: 600, Loss: 0.010198627732415843\nEpoch 74 Validation Loss: 0.0018\nEpoch: 75, Batch: 100, Loss: 0.007553882604021851\nEpoch: 75, Batch: 200, Loss: 0.0070770747099118125\nEpoch: 75, Batch: 300, Loss: 0.013516156103590334\nEpoch: 75, Batch: 400, Loss: 0.008864209614424965\nEpoch: 75, Batch: 500, Loss: 0.010894165983014546\nEpoch: 75, Batch: 600, Loss: 0.007981796201038378\nEpoch 75 Validation Loss: 0.0025\nEpoch: 76, Batch: 100, Loss: 0.014383743028550668\nEpoch: 76, Batch: 200, Loss: 0.010311597205825364\nEpoch: 76, Batch: 300, Loss: 0.008102781978094243\nEpoch: 76, Batch: 400, Loss: 0.009508781023637312\nEpoch: 76, Batch: 500, Loss: 0.013270313692860327\nEpoch: 76, Batch: 600, Loss: 0.017081563670517427\nEpoch 76 Validation Loss: 0.0041\nEpoch: 77, Batch: 100, Loss: 0.010684779053171951\nEpoch: 77, Batch: 200, Loss: 0.011760079472938969\nEpoch: 77, Batch: 300, Loss: 0.012785316824985102\nEpoch: 77, Batch: 400, Loss: 0.011303120001738307\nEpoch: 77, Batch: 500, Loss: 0.013732899341039797\nEpoch: 77, Batch: 600, Loss: 0.013235020879569674\nEpoch 77 Validation Loss: 0.0031\nEpoch: 78, Batch: 100, Loss: 0.011881358869770793\nEpoch: 78, Batch: 200, Loss: 0.010046131314063587\nEpoch: 78, Batch: 300, Loss: 0.010225405699621888\nEpoch: 78, Batch: 400, Loss: 0.007620064014368211\nEpoch: 78, Batch: 500, Loss: 0.014948020496751724\nEpoch: 78, Batch: 600, Loss: 0.005762696312834237\nEpoch 78 Validation Loss: 0.0046\nEpoch: 79, Batch: 100, Loss: 0.012860164052262917\nEpoch: 79, Batch: 200, Loss: 0.010680569620629967\nEpoch: 79, Batch: 300, Loss: 0.0093123229826071\nEpoch: 79, Batch: 400, Loss: 0.01118680570516858\nEpoch: 79, Batch: 500, Loss: 0.010940371082556908\nEpoch: 79, Batch: 600, Loss: 0.014547670366057445\nEpoch 79 Validation Loss: 0.0017\nEpoch: 80, Batch: 100, Loss: 0.016187339375856025\nEpoch: 80, Batch: 200, Loss: 0.010586564962470674\nEpoch: 80, Batch: 300, Loss: 0.01125809978035818\nEpoch: 80, Batch: 400, Loss: 0.006782943623386472\nEpoch: 80, Batch: 500, Loss: 0.01888049079030452\nEpoch: 80, Batch: 600, Loss: 0.012495526635616443\nEpoch 80 Validation Loss: 0.0018\nEarly stopping at epoch 80\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images.float())\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nval_acc = 100 * correct / total\nprint(f'Epoch: {epoch + 1}, Validation Accuracy: {val_acc:.2f}%')\n\nprint('Finished Training')\n\n# Test prediction\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for data in test_loader:\n        data = data.to(device)\n        outputs = model(data.float())\n        _, predicted = torch.max(outputs, 1)\n        predictions.extend(predicted.cpu().tolist())\n\nsubmission = pd.DataFrame({\n    'ImageId': range(1, len(predictions) + 1),\n    'Label': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T10:10:50.098609Z","iopub.execute_input":"2026-01-06T10:10:50.098994Z","iopub.status.idle":"2026-01-06T10:11:20.345818Z","shell.execute_reply.started":"2026-01-06T10:10:50.098963Z","shell.execute_reply":"2026-01-06T10:11:20.344707Z"}},"outputs":[{"name":"stdout","text":"Epoch: 80, Validation Accuracy: 99.88%\nFinished Training\nSubmission saved!\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import zipfile\n\nwith zipfile.ZipFile('submission.zip', 'w') as z:\n    z.write('submission.csv')\n\nprint(\"Zipped!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T10:11:20.347581Z","iopub.execute_input":"2026-01-06T10:11:20.347892Z","iopub.status.idle":"2026-01-06T10:11:20.355943Z","shell.execute_reply.started":"2026-01-06T10:11:20.347861Z","shell.execute_reply":"2026-01-06T10:11:20.354830Z"}},"outputs":[{"name":"stdout","text":"Zipped!\n","output_type":"stream"}],"execution_count":48}]}